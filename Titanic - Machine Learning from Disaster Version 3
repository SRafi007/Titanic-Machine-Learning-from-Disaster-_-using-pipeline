{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn import set_config\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.base import BaseEstimator, TransformerMixin\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-04T17:26:30.319452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/titanic/train.csv')\ntest=pd.read_csv('/kaggle/input/titanic/test.csv')\nX_copy=train.copy()\nY_copy=test.copy()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def explore_data(data):\n    print(f\"Data Shape: {data.shape}\")\n    print(f\"Missing Values:\\n{data.isnull().sum()}\")\n    print(f\"Data Types:\\n{data.dtypes}\")\n    print(f\"Descriptive Statistics:\\n{data.describe()}\")\n    print(f\"Unique Values:\\n{data.nunique()}\")\n    \n    # Visualize missing values\n    plt.figure(figsize=(10, 6))\n    sns.heatmap(data.isnull(), cbar=False, cmap='viridis')\n    plt.title('Missing Values Heatmap')\n    plt.show()\n    \n    # Correlation matrix\n    cols=data.select_dtypes(include=['number']).columns\n    plt.figure(figsize=(12, 8))\n    sns.heatmap(data[cols].corr(), annot=True, fmt=\".2f\")\n    plt.title('Correlation Matrix')\n    plt.show()\n    \n    # Distribution of numerical features\n    data.hist(bins=20, figsize=(20, 15), edgecolor='black')\n    plt.show()\n    \n    \n# Explore training data\nexplore_data(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_copy['Title'] = X_copy['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\nplt.figure(figsize=(10, 6))\nsns.countplot(y='Title', data=X_copy, order=X_copy['Title'].value_counts().index)\nplt.title('Count Plot of Title')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_copy['Title'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Data_cleaning(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        \n        X=X.drop(['PassengerId'],axis=1)\n        \n        # Fill missing Age values with median value\n        X.fillna(X['Age'].median(), inplace=True)\n        \n\n        # Fill missing Embarked values with mode\n        X.fillna(X['Embarked'].mode()[0], inplace=True)\n\n        # Fill missing Fare value in test set with median\n        X.fillna(X['Fare'].median(), inplace=True)\n\n        # Create a new feature 'Has_Cabin' indicating whether a passenger has a Cabin\n        #X['Has_Cabin'] = X['Cabin'].fillna(0, inplace=True)\n        X['Has_Cabin'] = X['Cabin'].apply(lambda x: 1 if x==0 else 0)\n        \n        return X\n    \n\n    \n    \n    \nclass new_feature(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform (self, X):\n        # Create new feature FamilySize\n        X['FamilySize'] = X['SibSp'] + X['Parch'] + 1\n\n        # Create new feature IsAlone\n        X['IsAlone'] = X['FamilySize'].apply(lambda x: 1 if x == 1 else 0)\n\n        # Extract titles from names\n        #data['Title'] = data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n        X['Title'] = X['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\n        X['Title'] = X['Title'].replace(['Lady', 'Mlle','Capt', 'Mme',\n                                               'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'the Countess', 'Ms'], 'Rare')\n        X['Title'] = X['Title'].replace('Master', 'Master')\n        X['Title'] = X['Title'].replace(['Miss','Mrs'], 'Lady')\n        X['Title'] = X['Title'].replace('Mr', 'Mr')\n\n        # Convert titles to numeric\n        title_mapping = {\"Mr\": 1, \"Lady\": 2, \"Master\": 3, \"Rare\": 4 }\n        X['Title'] = X['Title'].map(title_mapping)\n        X['Title'] = X['Title'].fillna(0)\n\n        # Fare binning\n        X['FareBin'] = pd.qcut(X['Fare'], 4)\n\n        # Age binning\n        X['AgeBin'] = pd.cut(X['Age'].astype(int), 5)\n\n        # Convert categorical bins to numeric\n        from sklearn.preprocessing import LabelEncoder\n\n        label = LabelEncoder()\n        X['FareBin_Code'] = label.fit_transform(X['FareBin'])\n        X['AgeBin_Code'] = label.fit_transform(X['AgeBin'])\n\n        # Drop unnecessary columns\n        drop_columns = ['Cabin', 'Ticket', 'Name', 'FareBin', 'AgeBin']\n        X.drop(columns=drop_columns, inplace=True)\n        \n        return X\n    \n    \n    \nclass data_encoding(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform (self, X):\n        X = pd.get_dummies(X, columns=['Sex'], drop_first=True)\n        X = pd.get_dummies(X, columns=['Embarked'], drop_first=True)\n\n        bool_columns=X.select_dtypes(include=['bool']).columns\n        for col in bool_columns:\n            if col in X.columns:\n                X[col] = X[col].astype(int)\n                \n        return X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Cleaning_data=Data_cleaning()\nAdding_feature=new_feature()\nencoding_data=data_encoding()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_for_testing=make_pipeline(Cleaning_data,Adding_feature,encoding_data) \nX_copy=pipe_for_testing.fit_transform(X_copy)\nY_copy=pipe_for_testing.fit_transform(Y_copy)\nX_copy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mi_y = X_copy['Survived']\nmi_X = X_copy.drop(columns=['Survived'])\nmi_score = mutual_info_classif(mi_X , mi_y)\n\nfetures = []\nfor col, score in zip(mi_X.columns , mi_score):\n    print(col, \":\", score)\n    if(score>0):\n        fetures.append(col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"def plot_data(x_data, y_data):\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(data=X, x=x_data, y='Survived', hue= y_data)\n    plt.title('Fare Distribution by Pclass and Survival Status')\n    plt.xlabel(x_data)\n    plt.ylabel(y_data)\n    plt.legend(title='Survived')\n    plt.show()\n\n    # Swarm plot for Pclass and Fare, with hue as Survived\n    plt.figure(figsize=(10, 6))\n    sns.swarmplot(data=X, x=x_data, y=y_data, hue='Survived', dodge=True)\n    plt.title('Fare Distribution by Pclass and Survival Status')\n    plt.xlabel(x_data)\n    plt.ylabel(y_data)\n    plt.legend(title='Survived')\n    plt.show()\n\n    # Violin plot for Pclass and Fare, with hue as Survived\n    plt.figure(figsize=(10, 6))\n    sns.violinplot(data=X, x=x_data, y=y_data, hue='Survived', split=True)\n    plt.title('Fare Distribution by Pclass and Survival Status')\n    plt.xlabel(x_data)\n    plt.ylabel(y_data)\n    plt.legend(title='Survived')\n    plt.show()\n    \nplot_data( 'Pclass', 'FamilySize')","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:40:19.402857Z","iopub.execute_input":"2024-05-26T16:40:19.403314Z","iopub.status.idle":"2024-05-26T16:40:19.414917Z","shell.execute_reply.started":"2024-05-26T16:40:19.403279Z","shell.execute_reply":"2024-05-26T16:40:19.413643Z"}}},{"cell_type":"code","source":"fetures","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix=X[fetures].corr()\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\nplt.title('Correlation Heatmap including Categorical Features')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"def data_vis(X_data, x):\n    plt.figure(figsize=(10, 6))\n    plt.scatter(X[X_data], X[x], c=X['Survived'], cmap='viridis', alpha=0.6)\n    plt.colorbar(label='Survived')\n    plt.xlabel(X_data)\n    plt.ylabel(x)\n    plt.title(f'Scatter Plot of {X_data} vs {x} Colored by Survival')\n    plt.show()\n    \nfor col in X.columns:    \n    data_vis('Survived', col)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T20:15:41.313635Z","iopub.execute_input":"2024-05-28T20:15:41.314103Z","iopub.status.idle":"2024-05-28T20:15:49.406594Z","shell.execute_reply.started":"2024-05-28T20:15:41.314068Z","shell.execute_reply":"2024-05-28T20:15:49.405404Z"}}},{"cell_type":"markdown","source":"# **Creating New Feature**","metadata":{}},{"cell_type":"code","source":"print(X.columns)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class reconditioning_feature(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X): \n        \n        #X['Pclass_Alone']=X.apply(lambda row:0 if row['Pclass']==3 else (0 if row['Pclass']*row['IsAlone']>1 else 1), axis=1)\n        X['SibSp']=X.apply(lambda row:5 if row['SibSp']>4 else row['SibSp'], axis=1)\n        #X['SibSp']=X.apply(lambda row:5 if row['SibSp']>4 else row['SibSp'], axis=1)\n        X['Parch'] = X['Parch'].apply(lambda x: 4 if x in [4, 6] else x)\n        X['FamilySize'] = X['FamilySize'].apply(lambda x: 0 if x > 7 else x)\n        return X\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reconditioning_feature=reconditioning_feature()\npipe2=make_pipeline(reconditioning_feature)\nX=pipe2.fit_transform(X)\ntest=pipe2.fit_transform(test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mi_y = X['Survived']\nmi_X = X.drop(columns=['Survived'])\nmi_score = mutual_info_classif(X.drop(columns=['Survived']) , X['Survived'])\n\nfetures = []\nfor col, score in zip(mi_X.columns , mi_score):\n    print(col, \":\", score)\n    if(score>0):\n        fetures.append(col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix=X.corr()\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\nplt.title('Correlation Heatmap including Categorical Features')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fetures","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Necessary imports\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nimport numpy as np\n\n# Load a sample dataset\nY = X['Survived']\nX= X[fetures]\n\n# Dictionary of classification models\nclassification_models = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"AdaBoost\": AdaBoostClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n    \"Naive Bayes\": GaussianNB(),\n    \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n    \"LightGBM\": lgb.LGBMClassifier(),\n    \"CatBoost\": CatBoostClassifier(verbose=0)\n}\n\n# Perform cross-validation and store the results\ncv_results = {}\n\nfor model_name, model in classification_models.items():\n    scores = cross_val_score(model, X, Y, cv=5)\n    cv_results[model_name] = scores\n    #print(f\"{model_name}: Mean Accuracy = {np.mean(scores):.4f}, Standard Deviation = {np.std(scores):.4f}\")\n\n# Print cross-validation results\nprint(\"\\nCross-validation scores:\")\nfor model_name, scores in cv_results.items():\n    print(f\"{model_name}: {scores}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=test[fetures]\ntest","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid to search\nparam_grid = {\n    'iterations': [100, 200, 300],\n    'learning_rate': [0.05, 0.1, 0.2],\n    'depth': [4, 6, 8]\n}\n\n# Create a CatBoost classifier\ncat = CatBoostClassifier(verbose=0)\n\n# Instantiate GridSearchCV\ngrid_search = GridSearchCV(estimator=cat, param_grid=param_grid, cv=5, scoring='accuracy')\n\n# Perform grid search\ngrid_search.fit(X, Y)\n\n# Get the best parameters and best score\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\n\nprint(\"Best Parameters:\", best_params)\nprint(\"Best Score:\", best_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = {\n    'iterations':200,  \n    'learning_rate': 0.2,\n    'depth': 4\n}\n\nbest_cat = CatBoostClassifier(verbose=0, **best_params)\n\nbest_cat.fit(X, Y)\n\ny_pred = best_cat.predict(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission=pd.read_csv('/kaggle/input/titanic/gender_submission.csv')\nsubmission=submission.drop(columns=['Survived'],axis=1)\nsubmission['Survived']=y_pred\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('Submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}